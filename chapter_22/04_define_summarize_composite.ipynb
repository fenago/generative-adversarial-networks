{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Implement Adversarial and L1 Loss\n", 
        "The discriminator model can be updated directly, whereas the generator model must be updated\n", 
        "via the discriminator model. This can be achieved by defining a new composite model in Keras\n", 
        "that connects the output of the generator model as input to the discriminator model. The\n", 
        "discriminator model can then predict whether a generated image is real or fake. We can update\n", 
        "the weights of the composite model in such a way that the generated image has the label of\n", 
        "real instead of fake, which will cause the generator weights to be updated towards generating a\n", 
        "better fake image. We can also mark the discriminator weights as not trainable in this context,\n", 
        "to avoid the misleading update. Additionally, the generator needs to be updated to better match\n", 
        "the targeted translation of the input image. This means that the composite model must also\n", 
        "output the generated image directly, allowing it to be compared to the target image. Therefore,\n", 
        "we can summarize the inputs and outputs of this composite model as follows:\n", 
        "\u277c Inputs: Source image\n", 
        "\u277c Outputs: Classification of real/fake, generated target image.\n", 
        "The weights of the generator will be updated via both adversarial loss via the discriminator\n", 
        "output and L1 loss via the direct image output. The loss scores are added together, where the\n", 
        "L1 loss is treated as a regularizing term and weighted via a hyperparameter called lambda (\u03bb),\n", 
        "set to 100."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining a composite model for training the generator model\n", 
        "from keras.optimizers import Adam\n", 
        "from keras.initializers import RandomNormal\n", 
        "from keras.models import Model\n", 
        "from keras.models import Input\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Conv2DTranspose\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.layers import Activation\n", 
        "from keras.layers import Concatenate\n", 
        "from keras.layers import Dropout\n", 
        "from keras.layers import BatchNormalization\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "\n", 
        "# define the discriminator model\n", 
        "def define_discriminator(image_shape):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# source image input\n", 
        "\tin_src_image = Input(shape=image_shape)\n", 
        "\t# target image input\n", 
        "\tin_target_image = Input(shape=image_shape)\n", 
        "\t# concatenate images channel-wise\n", 
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n", 
        "\t# C64\n", 
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C128\n", 
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C256\n", 
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C512\n", 
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# second last output layer\n", 
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# patch output\n", 
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n", 
        "\tpatch_out = Activation('sigmoid')(d)\n", 
        "\t# define model\n", 
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n", 
        "\t# compile model\n", 
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n", 
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n", 
        "\treturn model\n", 
        "\n", 
        "# define an encoder block\n", 
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# add downsampling layer\n", 
        "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n", 
        "\t# conditionally add batch normalization\n", 
        "\tif batchnorm:\n", 
        "\t\tg = BatchNormalization()(g, training=True)\n", 
        "\t# leaky relu activation\n", 
        "\tg = LeakyReLU(alpha=0.2)(g)\n", 
        "\treturn g\n", 
        "\n", 
        "# define a decoder block\n", 
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# add upsampling layer\n", 
        "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n", 
        "\t# add batch normalization\n", 
        "\tg = BatchNormalization()(g, training=True)\n", 
        "\t# conditionally add dropout\n", 
        "\tif dropout:\n", 
        "\t\tg = Dropout(0.5)(g, training=True)\n", 
        "\t# merge with skip connection\n", 
        "\tg = Concatenate()([g, skip_in])\n", 
        "\t# relu activation\n", 
        "\tg = Activation('relu')(g)\n", 
        "\treturn g\n", 
        "\n", 
        "# define the standalone generator model\n", 
        "def define_generator(image_shape=(256,256,3)):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# image input\n", 
        "\tin_image = Input(shape=image_shape)\n", 
        "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n", 
        "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n", 
        "\te2 = define_encoder_block(e1, 128)\n", 
        "\te3 = define_encoder_block(e2, 256)\n", 
        "\te4 = define_encoder_block(e3, 512)\n", 
        "\te5 = define_encoder_block(e4, 512)\n", 
        "\te6 = define_encoder_block(e5, 512)\n", 
        "\te7 = define_encoder_block(e6, 512)\n", 
        "\t# bottleneck, no batch norm and relu\n", 
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n", 
        "\tb = Activation('relu')(b)\n", 
        "\t# decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n", 
        "\td1 = decoder_block(b, e7, 512)\n", 
        "\td2 = decoder_block(d1, e6, 512)\n", 
        "\td3 = decoder_block(d2, e5, 512)\n", 
        "\td4 = decoder_block(d3, e4, 512, dropout=False)\n", 
        "\td5 = decoder_block(d4, e3, 256, dropout=False)\n", 
        "\td6 = decoder_block(d5, e2, 128, dropout=False)\n", 
        "\td7 = decoder_block(d6, e1, 64, dropout=False)\n", 
        "\t# output\n", 
        "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n", 
        "\tout_image = Activation('tanh')(g)\n", 
        "\t# define model\n", 
        "\tmodel = Model(in_image, out_image)\n", 
        "\treturn model\n", 
        "\n", 
        "# define the combined generator and discriminator model, for updating the generator\n", 
        "def define_gan(g_model, d_model, image_shape):\n", 
        "\t# make weights in the discriminator not trainable\n", 
        "\td_model.trainable = False\n", 
        "\t# define the source image\n", 
        "\tin_src = Input(shape=image_shape)\n", 
        "\t# connect the source image to the generator input\n", 
        "\tgen_out = g_model(in_src)\n", 
        "\t# connect the source input and generator output to the discriminator input\n", 
        "\tdis_out = d_model([in_src, gen_out])\n", 
        "\t# src image as input, generated image and classification output\n", 
        "\tmodel = Model(in_src, [dis_out, gen_out])\n", 
        "\t# compile model\n", 
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n", 
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n", 
        "\treturn model\n", 
        "\n", 
        "# define image shape\n", 
        "image_shape = (256,256,3)\n", 
        "# define the models\n", 
        "d_model = define_discriminator(image_shape)\n", 
        "g_model = define_generator(image_shape)\n", 
        "# define the composite model\n", 
        "gan_model = define_gan(g_model, d_model, image_shape)\n", 
        "# summarize the model\n", 
        "gan_model.summary()\n", 
        "# plot the model\n", 
        "plot_model(gan_model, to_file='gan_model_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('gan_model_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example first summarizes the composite model, showing the 256 \u00d7 256 image\n", 
        "input, the same shaped output from model 2 (the generator) and the PatchGAN classification\n", 
        "prediction from model 1 (the discriminator)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}