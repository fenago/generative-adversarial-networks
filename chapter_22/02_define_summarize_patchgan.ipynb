{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Implement the PatchGAN Discriminator Model\n", 
        "The discriminator model in the Pix2Pix GAN is implemented as a PatchGAN. The PatchGAN\n", 
        "is designed based on the size of the receptive field, sometimes called the effective receptive field.\n", 
        "The receptive field is the relationship between one output activation of the model to an area on\n", 
        "the input image (actually volume as it proceeded down the input channels). A PatchGAN with\n", 
        "the size 70 \u00d7 70 is used, which means that the output (or each output) of the model maps to a\n", 
        "70 \u00d7 70 square of the input image. In effect, a 70 \u00d7 70 PatchGAN will classify 70 \u00d7 70 patches\n", 
        "of the input image as real or fake."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The model is trained with a batch size of one image and the Adam version of stochastic\n", 
        "gradient descent is used with a small learning range and modest momentum. The loss for the\n", 
        "discriminator is weighted by 50% for each model update. Tying this all together, we can define\n", 
        "a function named define discriminator() that creates the 70 \u00d7 70 PatchGAN discriminator\n", 
        "model. The complete example of defining the model is listed below."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining a 70x70 patchgan discriminator model\n", 
        "from keras.optimizers import Adam\n", 
        "from keras.initializers import RandomNormal\n", 
        "from keras.models import Model\n", 
        "from keras.models import Input\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.layers import Activation\n", 
        "from keras.layers import Concatenate\n", 
        "from keras.layers import BatchNormalization\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "\n", 
        "# define the discriminator model\n", 
        "def define_discriminator(image_shape):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# source image input\n", 
        "\tin_src_image = Input(shape=image_shape)\n", 
        "\t# target image input\n", 
        "\tin_target_image = Input(shape=image_shape)\n", 
        "\t# concatenate images channel-wise\n", 
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n", 
        "\t# C64\n", 
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C128\n", 
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C256\n", 
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# C512\n", 
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# second last output layer\n", 
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\td = LeakyReLU(alpha=0.2)(d)\n", 
        "\t# patch output\n", 
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n", 
        "\tpatch_out = Activation('sigmoid')(d)\n", 
        "\t# define model\n", 
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n", 
        "\t# compile model\n", 
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n", 
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n", 
        "\treturn model\n", 
        "\n", 
        "# define image shape\n", 
        "image_shape = (256,256,3)\n", 
        "# create the model\n", 
        "model = define_discriminator(image_shape)\n", 
        "# summarize the model\n", 
        "model.summary()\n", 
        "# plot the model\n", 
        "plot_model(model, to_file='discriminator_model_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('discriminator_model_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}