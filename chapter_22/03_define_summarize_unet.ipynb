{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Implement the U-Net Generator Model\n", 
        "The generator model for the Pix2Pix GAN is implemented as a U-Net. The U-Net model is an\n", 
        "encoder-decoder model for image translation where skip connections are used to connect layers\n", 
        "in the encoder with corresponding layers in the decoder that have the same sized feature maps.\n", 
        "The encoder part of the model is comprised of convolutional layers that use a 2 \u00d7 2 stride to\n", 
        "downsample the input source image down to a bottleneck layer. The decoder part of the model\n", 
        "reads the bottleneck output and uses transpose convolutional layers to upsample to the required\n", 
        "output image size"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Skip connections are added between the layers with the same sized feature maps so that the\n", 
        "first downsampling layer is connected with the last upsampling layer, the second downsampling\n", 
        "layer is connected with the second last upsampling layer, and so on. The connections concatenate\n", 
        "the channels of the feature map in the downsampling layer with the feature map in the upsampling\n", 
        "layer.\n", 
        "\n", 
        "Unlike traditional generator models in the GAN architecture, the U-Net generator does not\n", 
        "take a point from the latent space as input. Instead, dropout layers are used as a source of\n", 
        "randomness both during training and when the model is used to make a prediction, e.g. generate\n", 
        "an image at inference time. Similarly, batch normalization is used in the same way during\n", 
        "training and inference, meaning that statistics are calculated for each batch and not fixed at\n", 
        "the end of the training process. This is referred to as instance normalization, specifically when\n", 
        "the batch size is set to 1 as it is with the Pix2Pix model."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example first summarizes the model. The output of the model summary was\n", 
        "omitted here for brevity. The model has a single input and output, but the skip connections\n", 
        "make the summary difficult to read. A plot of the model is created showing much the same\n", 
        "information in a graphical form. The model is complex, and the plot helps to understand the\n", 
        "skip connections and their impact on the number of filters in the decoder."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining a u-net encoder-decoder generator model\n", 
        "from keras.initializers import RandomNormal\n", 
        "from keras.models import Model\n", 
        "from keras.models import Input\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Conv2DTranspose\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.layers import Activation\n", 
        "from keras.layers import Concatenate\n", 
        "from keras.layers import Dropout\n", 
        "from keras.layers import BatchNormalization\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "\n", 
        "# define an encoder block\n", 
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# add downsampling layer\n", 
        "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n", 
        "\t# conditionally add batch normalization\n", 
        "\tif batchnorm:\n", 
        "\t\tg = BatchNormalization()(g, training=True)\n", 
        "\t# leaky relu activation\n", 
        "\tg = LeakyReLU(alpha=0.2)(g)\n", 
        "\treturn g\n", 
        "\n", 
        "# define a decoder block\n", 
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# add upsampling layer\n", 
        "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n", 
        "\t# add batch normalization\n", 
        "\tg = BatchNormalization()(g, training=True)\n", 
        "\t# conditionally add dropout\n", 
        "\tif dropout:\n", 
        "\t\tg = Dropout(0.5)(g, training=True)\n", 
        "\t# merge with skip connection\n", 
        "\tg = Concatenate()([g, skip_in])\n", 
        "\t# relu activation\n", 
        "\tg = Activation('relu')(g)\n", 
        "\treturn g\n", 
        "\n", 
        "# define the standalone generator model\n", 
        "def define_generator(image_shape=(256,256,3)):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# image input\n", 
        "\tin_image = Input(shape=image_shape)\n", 
        "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n", 
        "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n", 
        "\te2 = define_encoder_block(e1, 128)\n", 
        "\te3 = define_encoder_block(e2, 256)\n", 
        "\te4 = define_encoder_block(e3, 512)\n", 
        "\te5 = define_encoder_block(e4, 512)\n", 
        "\te6 = define_encoder_block(e5, 512)\n", 
        "\te7 = define_encoder_block(e6, 512)\n", 
        "\t# bottleneck, no batch norm and relu\n", 
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n", 
        "\tb = Activation('relu')(b)\n", 
        "\t# decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n", 
        "\td1 = decoder_block(b, e7, 512)\n", 
        "\td2 = decoder_block(d1, e6, 512)\n", 
        "\td3 = decoder_block(d2, e5, 512)\n", 
        "\td4 = decoder_block(d3, e4, 512, dropout=False)\n", 
        "\td5 = decoder_block(d4, e3, 256, dropout=False)\n", 
        "\td6 = decoder_block(d5, e2, 128, dropout=False)\n", 
        "\td7 = decoder_block(d6, e1, 64, dropout=False)\n", 
        "\t# output\n", 
        "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n", 
        "\tout_image = Activation('tanh')(g)\n", 
        "\t# define model\n", 
        "\tmodel = Model(in_image, out_image)\n", 
        "\treturn model\n", 
        "\n", 
        "# define image shape\n", 
        "image_shape = (256,256,3)\n", 
        "# create the model\n", 
        "model = define_generator(image_shape)\n", 
        "# summarize the model\n", 
        "model.summary()\n", 
        "# plot the model\n", 
        "plot_model(model, to_file='generator_model_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('generator_model_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example first summarizes the model. The output of the model summary was\n", 
        "omitted here for brevity. The model has a single input and output, but the skip connections\n", 
        "make the summary difficult to read. A plot of the model is created showing much the same\n", 
        "information in a graphical form. The model is complex, and the plot helps to understand the\n", 
        "skip connections and their impact on the number of filters in the decoder."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}