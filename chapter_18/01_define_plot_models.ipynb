{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Develop an InfoGAN for MNIST\n", 
        "In this section, we will take a closer look at the generator (g), discriminator (d), and auxiliary\n", 
        "models (q) and how to implement them in Keras. We will develop an InfoGAN implementation\n", 
        "for the MNIST dataset (described in Section 7.2), as was done in the InfoGAN paper. The\n", 
        "paper explores two versions; the first uses just categorical control codes and allows the model to\n", 
        "map one categorical variable to approximately one digit (although there is no ordering of the\n", 
        "digits by categorical variables)\n", 
        "\n", 
        "The paper also explores a version of the InfoGAN architecture with the one hot encoded\n", 
        "categorical variable (c1) and two continuous control variables (c2 and c3). The first continuous\n", 
        "variable is discovered to control the rotation of the digits and the second controls the thickness\n", 
        "of the digits."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# create and plot the infogan model for mnist\n", 
        "from keras.optimizers import Adam\n", 
        "from keras.models import Model\n", 
        "from keras.layers import Input\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import Reshape\n", 
        "from keras.layers import Flatten\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Conv2DTranspose\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.layers import BatchNormalization\n", 
        "from keras.layers import Activation\n", 
        "from keras.initializers import RandomNormal\n", 
        "from keras.utils.vis_utils import plot_model"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Next, we can define the discriminator and auxiliary models. The discriminator model is\n", 
        "trained in a standalone manner on real and fake images, as per a normal GAN. Neither the\n", 
        "generator nor the auxiliary models are fit directly; instead, they are fit as part of a composite\n", 
        "model. Both the discriminator and auxiliary models share the same input and feature extraction\n", 
        "layers but differ in their output layers. Therefore, it makes sense to define them both at the same\n", 
        "time. Again, there are many ways that this architecture could be implemented, but defining the\n", 
        "discriminator and auxiliary models as separate models first allows us later to combine them into\n", 
        "a larger GAN model directly via the functional API.\n", 
        "\n", 
        "The define discriminator() function below defines the discriminator and auxiliary models\n", 
        "and takes the cardinality of the categorical variable (e.g. number of values, such as 10) as an\n", 
        "input. The shape of the input image is also parameterized as a function argument and set\n", 
        "to the default value of the size of the MNIST images. The feature extraction layers involve\n", 
        "two downsampling layers, used instead of pooling layers as a best practice. Also following best\n", 
        "practice for DCGAN models, we use the LeakyReLU activation and batch normalization.\n", 
        "The discriminator model (d) has a single output node and predicts the probability of an\n", 
        "input image being real via the sigmoid activation function. The model is compiled as it will be\n", 
        "used in a standalone way, optimizing the binary cross-entropy function via the Adam version\n", 
        "of stochastic gradient descent with best practice learning rate and momentum. The auxiliary\n", 
        "model (q) has one node output for each value in the categorical variable and uses a softmax\n", 
        "activation function. A fully connected layer is added between the feature extraction layers and\n", 
        "the output layer, as was used in the InfoGAN paper. The model is not compiled as it is not for\n", 
        "or used in a standalone manner."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# define the standalone discriminator model\n", 
        "def define_discriminator(n_cat, in_shape=(28,28,1)):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# image input\n", 
        "\tin_image = Input(shape=in_shape)\n", 
        "\t# downsample to 14x14\n", 
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n", 
        "\td = LeakyReLU(alpha=0.1)(d)\n", 
        "\t# downsample to 7x7\n", 
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n", 
        "\td = LeakyReLU(alpha=0.1)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\t# normal\n", 
        "\td = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n", 
        "\td = LeakyReLU(alpha=0.1)(d)\n", 
        "\td = BatchNormalization()(d)\n", 
        "\t# flatten feature maps\n", 
        "\td = Flatten()(d)\n", 
        "\t# real/fake output\n", 
        "\tout_classifier = Dense(1, activation='sigmoid')(d)\n", 
        "\t# define d model\n", 
        "\td_model = Model(in_image, out_classifier)\n", 
        "\t# compile d model\n", 
        "\td_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n", 
        "\t# create q model layers\n", 
        "\tq = Dense(128)(d)\n", 
        "\tq = BatchNormalization()(q)\n", 
        "\tq = LeakyReLU(alpha=0.1)(q)\n", 
        "\t# q model output\n", 
        "\tout_codes = Dense(n_cat, activation='softmax')(q)\n", 
        "\t# define q model\n", 
        "\tq_model = Model(in_image, out_codes)\n", 
        "\treturn d_model, q_model\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Let\u2019s start off by developing the generator model as a deep convolutional neural network (e.g.\n", 
        "a DCGAN). The model could take the noise vector (z) and control vector (c) as separate inputs\n", 
        "and concatenate them before using them as the basis for generating the image. Alternately,\n", 
        "the vectors can be concatenated beforehand and provided to a single input layer in the model.\n", 
        "The approaches are equivalent and we will use the latter in this case to keep the model simple.\n", 
        "The define generator() function below defines the generator model and takes the size of the\n", 
        "input vector as an argument.\n", 
        "\n", 
        "A fully connected layer takes the input vector and produces a sufficient number of activations\n", 
        "to create 512 7 \u00d7 7 feature maps from which the activations are reshaped. These then pass\n", 
        "through a normal convolutional layer with 1\u00d71 stride, then two subsequent upsampling transpose\n", 
        "convolutional layers with a 2 \u00d7 2 stride first to 14 \u00d7 14 feature maps then to the desired 1\n", 
        "channel 28 \u00d7 28 feature map output with pixel values in the range [-1,-1] via a Tanh activation\n", 
        "function. Good generator configuration heuristics are as follows, including a random Gaussian\n", 
        "weight initialization, ReLU activations in the hidden layers, and use of batch normalization."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# define the standalone generator model\n", 
        "def define_generator(gen_input_size):\n", 
        "\t# weight initialization\n", 
        "\tinit = RandomNormal(stddev=0.02)\n", 
        "\t# image generator input\n", 
        "\tin_lat = Input(shape=(gen_input_size,))\n", 
        "\t# foundation for 7x7 image\n", 
        "\tn_nodes = 512 * 7 * 7\n", 
        "\tgen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n", 
        "\tgen = Activation('relu')(gen)\n", 
        "\tgen = BatchNormalization()(gen)\n", 
        "\tgen = Reshape((7, 7, 512))(gen)\n", 
        "\t# normal\n", 
        "\tgen = Conv2D(128, (4,4), padding='same', kernel_initializer=init)(gen)\n", 
        "\tgen = Activation('relu')(gen)\n", 
        "\tgen = BatchNormalization()(gen)\n", 
        "\t# upsample to 14x14\n", 
        "\tgen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n", 
        "\tgen = Activation('relu')(gen)\n", 
        "\tgen = BatchNormalization()(gen)\n", 
        "\t# upsample to 28x28\n", 
        "\tgen = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n", 
        "\t# tanh output\n", 
        "\tout_layer = Activation('tanh')(gen)\n", 
        "\t# define model\n", 
        "\tmodel = Model(in_lat, out_layer)\n", 
        "\treturn model\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Next, we can define the composite GAN model. This model uses all submodels and is\n", 
        "the basis for training the weights of the generator model. The define gan() function below\n", 
        "implements this and defines and returns the model, taking the three submodels as input. The\n", 
        "discriminator is trained in a standalone manner as mentioned, therefore all weights of the\n", 
        "discriminator are set as not trainable (in this context only). The output of the generator model\n", 
        "is connected to the input of the discriminator model, and to the input of the auxiliary model.\n", 
        "\n", 
        "This creates a new composite model that takes a [noise + control] vector as input, that\n", 
        "then passes through the generator to produce an image. The image then passes through the\n", 
        "discriminator model to produce a classification and through the auxiliary model to produce a\n", 
        "prediction of the control variables. The model has two output layers that need to be trained\n", 
        "with different loss functions. Binary cross-entropy loss is used for the discriminator output, as\n", 
        "we did when compiling the discriminator for standalone use, and mutual information loss is\n", 
        "used for the auxiliary model, which, in this case, can be implemented directly as categorical\n", 
        "cross-entropy and achieve the desired result."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# define the combined discriminator, generator and q network model\n", 
        "def define_gan(g_model, d_model, q_model):\n", 
        "\t# make weights in the discriminator (some shared with the q model) as not trainable\n", 
        "\td_model.trainable = False\n", 
        "\t# connect g outputs to d inputs\n", 
        "\td_output = d_model(g_model.output)\n", 
        "\t# connect g outputs to q inputs\n", 
        "\tq_output = q_model(g_model.output)\n", 
        "\t# define composite model\n", 
        "\tmodel = Model(g_model.input, [d_output, q_output])\n", 
        "\t# compile model\n", 
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n", 
        "\tmodel.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer=opt)\n", 
        "\treturn model\n", 
        "\n", 
        "# number of values for the categorical control code\n", 
        "n_cat = 10\n", 
        "# size of the latent space\n", 
        "latent_dim = 62\n", 
        "# create the discriminator\n", 
        "d_model, q_model = define_discriminator(n_cat)\n", 
        "# create the generator\n", 
        "gen_input_size = latent_dim + n_cat\n", 
        "g_model = define_generator(gen_input_size)\n", 
        "# create the gan\n", 
        "gan_model = define_gan(g_model, d_model, q_model)\n", 
        "# plot the model\n", 
        "plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('gan_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example creates all three models, then creates the composite GAN model and\n", 
        "saves a plot of the model architecture.\n", 
        "\n", 
        "Note: Creating a plot of the model assumes that the pydot and graphviz libraries are\n", 
        "installed. If this is a problem, you can comment out the import statement and the function call\n", 
        "for plot model().\n", 
        "\n", 
        "The plot shows all of the detail for the generator model and the compressed description\n", 
        "of the discriminator and auxiliary models. Importantly, note the shape of the output of the\n", 
        "discriminator as a single node for predicting whether the image is real or fake, and the 10\n", 
        "nodes for the auxiliary model to predict the categorical control code. Recall that this composite\n", 
        "model will only be used to update the model weights of the generator and auxiliary models,\n", 
        "and all weights in the discriminator model will remain untrainable, i.e. only updated when the\n", 
        "standalone discriminator model is updated."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}