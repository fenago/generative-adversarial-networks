{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Define and Train the Discriminator Model\n", 
        "The first step is to define the discriminator model. The model must take a sample image from\n", 
        "our dataset as input and output a classification prediction as to whether the sample is real or\n", 
        "fake. This is a binary classification problem:\n", 
        "\n", 
        "- Inputs: Image with one channel and 28 \u00d7 28 pixels in size.\n", 
        "- Outputs: Binary classification, likelihood the sample is real (or fake).\n", 
        "\n", 
        "The discriminator model has two convolutional layers with 64 filters each, a small kernel size\n", 
        "of 3, and larger than normal stride of 2. The model has no pooling layers and a single node in\n", 
        "the output layer with the sigmoid activation function to predict whether the input sample is real7.3. How to Define and Train the Discriminator Model 99\n", 
        "or fake. The model is trained to minimize the binary cross-entropy loss function, appropriate\n", 
        "for binary classification. We will use some best practices in defining the discriminator model,\n", 
        "such as the use of LeakyReLU instead of ReLU, using Dropout, and using the Adam version of\n", 
        "stochastic gradient descent with a learning rate of 0.0002 and a momentum of 0.5. The function\n", 
        "define discriminator() below defines the discriminator model and parametrizes the size of\n", 
        "the input image."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining the discriminator model\n", 
        "from keras.models import Sequential\n", 
        "from keras.optimizers import Adam\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Flatten\n", 
        "from keras.layers import Dropout\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "\n", 
        "# define the standalone discriminator model\n", 
        "def define_discriminator(in_shape=(28,28,1)):\n", 
        "\tmodel = Sequential()\n", 
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\tmodel.add(Dropout(0.4))\n", 
        "\tmodel.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\tmodel.add(Dropout(0.4))\n", 
        "\tmodel.add(Flatten())\n", 
        "\tmodel.add(Dense(1, activation='sigmoid'))\n", 
        "\t# compile model\n", 
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n", 
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n", 
        "\treturn model\n", 
        "\n", 
        "# define model\n", 
        "model = define_discriminator()\n", 
        "# summarize the model\n", 
        "model.summary()\n", 
        "# plot the model\n", 
        "plot_model(model, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('discriminator_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example first summarizes the model architecture, showing the input and output\n", 
        "from each layer. We can see that the aggressive 2 \u00d7 2 stride acts to downsample the input\n", 
        "image, first from 28 \u00d7 28 to 14 \u00d7 14, then to 7 \u00d7 7, before the model makes an output prediction.\n", 
        "This pattern is by design as we do not use pooling layers and use the large stride as to achieve\n", 
        "a similar downsampling effect. We will see a similar pattern, but in reverse, in the generator\n", 
        "model in the next section."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}