{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Define and Use the Generator Model\n", 
        "The generator model is responsible for creating new, fake but plausible images of handwritten\n", 
        "digits. It does this by taking a point from the latent space as input and outputting a square7.4. How to Define and Use the Generator Model 106\n", 
        "grayscale image. The latent space is an arbitrarily defined vector space of Gaussian-distributed\n", 
        "values, e.g. 100 dimensions. It has no meaning, but by drawing points from this space randomly\n", 
        "and providing them to the generator model during training, the generator model will assign\n", 
        "meaning to the latent points. At the end of training, the latent vector space represents a\n", 
        "compressed representation of the output space, MNIST images, that only the generator knows\n", 
        "how to turn into plausible MNIST images.\n", 
        "\n", 
        "- Inputs: Point in latent space, e.g. a 100 element vector of Gaussian random numbers.\n", 
        "- Outputs: Two-dimensional square grayscale image of 28 \u00d7 28 pixels with pixel values in\n", 
        "[0,1].\n", 
        "\n", 
        "We don\u2019t have to use a 100 element vector as input; it is a round number and widely used,\n", 
        "but I would expect that 10, 50, or 500 would work just as well. Developing a generator model\n", 
        "requires that we transform a vector from the latent space with, 100 dimensions to a 2D array\n", 
        "with 28\u00d728 or 784 values. There are a number of ways to achieve this but there is one approach\n", 
        "that has proven effective at deep convolutional generative adversarial networks. It involves two\n", 
        "main elements. The first is a Dense layer as the first hidden layer that has enough nodes to\n", 
        "represent a low-resolution version of the output image. Specifically, an image half the size (one\n", 
        "quarter the area) of the output image would be 14 \u00d7 14 or 196 nodes, and an image one quarter\n", 
        "the size (one eighth the area) would be 7 \u00d7 7 or 49 nodes.\n", 
        "\n", 
        "We don\u2019t just want one low-resolution version of the image; we want many parallel versions\n", 
        "or interpretations of the input. This is a pattern in convolutional neural networks where we\n", 
        "have many parallel filters resulting in multiple parallel activation maps, called feature maps,\n", 
        "with different interpretations of the input. We want the same thing in reverse: many parallel\n", 
        "versions of our output with different learned features that can be collapsed in the output layer\n", 
        "into a final image. The model needs space to invent, create, or generate. Therefore, the first\n", 
        "hidden layer, the Dense layer needs enough nodes for multiple low-resolution versions of our\n", 
        "output image, such as 128."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining and using the generator model\n", 
        "from numpy import zeros\n", 
        "from numpy.random import randn\n", 
        "from keras.models import Sequential\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import Reshape\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Conv2DTranspose\n", 
        "from keras.layers import LeakyReLU\n", 
        "%matplotlib notebook\n", 
        "from matplotlib import pyplot\n", 
        "\n", 
        "# define the standalone generator model\n", 
        "def define_generator(latent_dim):\n", 
        "\tmodel = Sequential()\n", 
        "\t# foundation for 7x7 image\n", 
        "\tn_nodes = 128 * 7 * 7\n", 
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\tmodel.add(Reshape((7, 7, 128)))\n", 
        "\t# upsample to 14x14\n", 
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\t# upsample to 28x28\n", 
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n", 
        "\treturn model\n", 
        "\n", 
        "# generate points in latent space as input for the generator\n", 
        "def generate_latent_points(latent_dim, n_samples):\n", 
        "\t# generate points in the latent space\n", 
        "\tx_input = randn(latent_dim * n_samples)\n", 
        "\t# reshape into a batch of inputs for the network\n", 
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n", 
        "\treturn x_input\n", 
        "\n", 
        "# use the generator to generate n fake examples, with class labels\n", 
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n", 
        "\t# generate points in latent space\n", 
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n", 
        "\t# predict outputs\n", 
        "\tX = g_model.predict(x_input)\n", 
        "\t# create 'fake' class labels (0)\n", 
        "\ty = zeros((n_samples, 1))\n", 
        "\treturn X, y\n", 
        "\n", 
        "# size of the latent space\n", 
        "latent_dim = 100\n", 
        "# define the discriminator model\n", 
        "model = define_generator(latent_dim)\n", 
        "# generate samples\n", 
        "n_samples = 25\n", 
        "X, _ = generate_fake_samples(model, latent_dim, n_samples)\n", 
        "# plot the generated samples\n", 
        "for i in range(n_samples):\n", 
        "\t# define subplot\n", 
        "\tpyplot.subplot(5, 5, 1 + i)\n", 
        "\t# turn off axis labels\n", 
        "\tpyplot.axis('off')\n", 
        "\t# plot single image\n", 
        "\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n", 
        "# show the figure\n", 
        "pyplot.show()"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Running the example summarizes the layers of the model and their output shape. We can\n", 
        "see that, as designed, the first hidden layer has 6,272 parameters or 128 \u00d7 7 \u00d7 7, the activations\n", 
        "of which are reshaped into 128 7 \u00d7 7 feature maps. The feature maps are then upscaled via\n", 
        "the two Conv2DTranspose layers to the desired output shape of 28 \u00d7 28, until the output layer,\n", 
        "where a single activation map is output."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}