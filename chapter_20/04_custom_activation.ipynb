{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## Stacked Discriminator Models With Shared Weights\n", 
        "A final approach is very similar to the prior two semi-supervised approaches and involves creating\n", 
        "separate logical unsupervised and supervised models but attempts to reuse the output layers of\n", 
        "one model to feed as input into another model. The approach is based on the definition of the\n", 
        "semi-supervised model in the 2016 paper by Tim Salimans, et al. from OpenAI titled Improved\n", 
        "Techniques for Training GANs. In the paper, they describe an efficient implementation, where\n", 
        "first the supervised model is created with K output classes and a softmax activation function.\n", 
        "The unsupervised model is then defined that takes the output of the supervised model prior to\n", 
        "the softmax activation, then calculates a normalized sum of the exponential outputs.\n", 
        "\n", 
        "To make this clearer, we can implement this activation function in NumPy and run some\n", 
        "sample activations through it to see what happens. The complete example is listed below."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of custom activation function\n", 
        "import numpy as np\n", 
        "\n", 
        "# custom activation function\n", 
        "def custom_activation(output):\n", 
        "\tlogexpsum = np.sum(np.exp(output))\n", 
        "\tresult = logexpsum / (logexpsum + 1.0)\n", 
        "\treturn result\n", 
        "\n", 
        "# all -10s\n", 
        "output = np.asarray([-10.0, -10.0, -10.0])\n", 
        "print(custom_activation(output))\n", 
        "# all -1s\n", 
        "output = np.asarray([-1.0, -1.0, -1.0])\n", 
        "print(custom_activation(output))\n", 
        "# all 0s\n", 
        "output = np.asarray([0.0, 0.0, 0.0])\n", 
        "print(custom_activation(output))\n", 
        "# all 1s\n", 
        "output = np.asarray([1.0, 1.0, 1.0])\n", 
        "print(custom_activation(output))\n", 
        "# all 10s\n", 
        "output = np.asarray([10.0, 10.0, 10.0])\n", 
        "print(custom_activation(output))"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember, the output of the unsupervised model prior to the softmax activation function\n", 
        "will be the activations of the nodes directly. They will be small positive or negative values, but\n", 
        "not normalized, as this would be performed by the softmax activation. The custom activation\n", 
        "function will output a value between 0.0 and 1.0. A value close to 0.0 is output for a small or\n", 
        "negative activation and a value close to 1.0 for a positive or large activation. We can see this\n", 
        "when we run the example."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}