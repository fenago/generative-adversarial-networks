{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "## How to Define and Use the Generator Model\n", 
        "The generator model is responsible for creating new, fake, but plausible small photographs of\n", 
        "objects. It does this by taking a point from the latent space as input and outputting a square\n", 
        "color image. The latent space is an arbitrarily defined vector space of Gaussian-distributed\n", 
        "values, e.g. 100 dimensions. It has no meaning, but by drawing points from this space randomly\n", 
        "and providing them to the generator model during training, the generator model will assign\n", 
        "meaning to the latent points and, in turn, the latent space, until, at the end of training, the\n", 
        "latent vector space represents a compressed representation of the output space, CIFAR-10\n", 
        "images, that only the generator knows how to turn into plausible CIFAR-10 images.\n", 
        "\u277c Inputs: Point in latent space, e.g. a 100-element vector of Gaussian random numbers.\n", 
        "\u277c Outputs: Two-dimensional square color image (3 channels) of 32 \u00d7 32 pixels with pixel\n", 
        "values in [-1,1].\n", 
        "\n", 
        "We don\u2019t have to use a 100 element vector as input; it is a round number and widely used,\n", 
        "but I would expect that 10, 50, or 500 would work just as well. Developing a generator model\n", 
        "requires that we transform a vector from the latent space with, 100 dimensions to a 2D array\n", 
        "with 32 \u00d7 32 \u00d7 3, or 3,072 values. There are a number of ways to achieve this, but there is one\n", 
        "approach that has proven effective on deep convolutional generative adversarial networks. It\n", 
        "involves two main elements. The first is a Dense layer as the first hidden layer that has enough\n", 
        "nodes to represent a low-resolution version of the output image. Specifically, an image half the\n", 
        "size (one quarter the area) of the output image would be 16x16x3, or 768 nodes, and an image\n", 
        "one quarter the size (one eighth the area) would be 8 \u00d7 8 \u00d7 3, or 192 nodes.\n", 
        "\n", 
        "With some experimentation, I have found that a smaller low-resolution version of the image\n", 
        "works better. Therefore, we will use 4\u00d74\u00d73, or 48 nodes. We don\u2019t just want one low-resolution\n", 
        "version of the image; we want many parallel versions or interpretations of the input. This is\n", 
        "a pattern in convolutional neural networks where we have many parallel filters resulting in\n", 
        "multiple parallel activation maps, called feature maps, with different interpretations of the input.\n", 
        "We want the same thing in reverse: many parallel versions of our output with different learned\n", 
        "features that can be collapsed in the output layer into a final image. The model needs space\n", 
        "to invent, create, or generate. Therefore, the first hidden layer, the Dense layer, needs enough\n", 
        "nodes for multiple versions of our output image, such as 256."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# example of defining the generator model\n", 
        "from keras.models import Sequential\n", 
        "from keras.layers import Dense\n", 
        "from keras.layers import Reshape\n", 
        "from keras.layers import Conv2D\n", 
        "from keras.layers import Conv2DTranspose\n", 
        "from keras.layers import LeakyReLU\n", 
        "from keras.utils.vis_utils import plot_model\n", 
        "\n", 
        "# define the standalone generator model\n", 
        "def define_generator(latent_dim):\n", 
        "\tmodel = Sequential()\n", 
        "\t# foundation for 4x4 image\n", 
        "\tn_nodes = 256 * 4 * 4\n", 
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\tmodel.add(Reshape((4, 4, 256)))\n", 
        "\t# upsample to 8x8\n", 
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\t# upsample to 16x16\n", 
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\t# upsample to 32x32\n", 
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n", 
        "\tmodel.add(LeakyReLU(alpha=0.2))\n", 
        "\t# output layer\n", 
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n", 
        "\treturn model\n", 
        "\n", 
        "# define the size of the latent space\n", 
        "latent_dim = 100\n", 
        "# define the generator model\n", 
        "model = define_generator(latent_dim)\n", 
        "# summarize the model\n", 
        "model.summary()\n", 
        "# plot the model\n", 
        "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "from PIL import Image\n", 
        "from IPython.display import display # to display images\n", 
        "\n", 
        "image = Image.open('generator_plot.png')\n", 
        "display(image)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}